# -*- coding: utf-8 -*-
"""flickering algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1brosRJLzQhrK7_YZLW0CED0DqLc9S8Tp
"""

pip install ruptures

#import ruptures as rpt
import numpy as np
import random
import matplotlib.pyplot as plt

def polynomial_gen(a,b,c,d,f,g,x):
  x = np.array(x)
  y = (-f*(x**7))+(d*(x**6))-(c*(x**5))+(b*(x**4))-(a*(x**3))+(g*x)
  return y

def find_roots(a, b, c, d, f, g,p):
    # Coefficients in descending order of powers
    coeffs = [-f, d, -c, b, -a, 0, g-1, p]
    # Find roots
    roots = np.roots(coeffs)
    real_roots = roots[np.isreal(roots)].real
    return real_roots

def coeff_gen():
  f = random.uniform(0,2)
  d = random.uniform(-f,f)
  c = random.uniform(0,2)
  b = random.uniform(-c,c)
  a = random.uniform(0,2)
  g = random.uniform(1,3)
  return a,b,c,d,f,g

a, b, c, d, f, g = coeff_gen()
p=0  # Example coefficients
roots = find_roots(a, b, c, d, f, g,p)


def find_derivative_roots(a,b,c,d,f,g):
    coeffs = [-7*f, 6*d, -5*c, 4*b, -3*a, 0, g-1]
    # Find roots
    roots = np.roots(coeffs)
    real_roots = roots[np.isreal(roots)].real
    return real_roots


derivative_roots = find_derivative_roots(a,b,c,d,f,g)


def find_p_star(x,y):
  return x-y



def if_correct(roots,derivative_roots):
  positive__derivative_roots = [root for root in derivative_roots if root > 0]
  positive_root = [root for root in roots if root > 0]
  if len(roots)==3 and positive__derivative_roots<positive_root:
    return True
  else:
    return False


def simulate(steps,p_star,p_0,x_0,a,b,c,d,f,g):

      X = np.zeros(steps)
      X[0] = x_0
      dt = 10**-2
      p = p_0
      delta_p = (p_star - p_0)/steps
      for i in range(0,steps-1):
        X[i+1] = X[i] + ((polynomial_gen(a,b,c,d,f,g,X[i])-X[i]+p)*dt) + (np.sqrt(dt)*sigma*np.random.normal(0,1))
        p = p + delta_p


      return X

def z_score(X):
  return (X-np.mean(X))/np.std(X)

def rpt_and_return(X,total_count,total_needed):
    algo = rpt.Pelt(model="rbf").fit(X)  # You can try other models: "l1", "l2", "normal", etc.
    breakpoints = algo.predict(pen=30)
    Training_set = {}  # Initialize as a dictionary
    change_points = breakpoints[:-1]
    count = 0
    length = 5000
    for point in change_points:
      if point - length >= 0 and point + length < len(X):  # Check to avoid index out of bounds
          if np.abs(X[point+length] - X[point-length]) > 1:
              if total_count < total_needed:
                Training_set[count] = z_score(X[point-length:point+length])  # Store in the dictionary
                count += 1
              else:
                break
    return Training_set,count

def rpt_and_reject(X):
    algo = rpt.Pelt(model="rbf").fit(X)  # You can try other models: "l1", "l2", "normal", etc.
    breakpoints = algo.predict(pen=30)
    change_points = breakpoints[:-1]
    result = 0 if len(change_points)==0 else 1
    return result

total_needed1 = 2 #3000
total_count = 0
All_training_set1 = {}  # Initialize as a dictionary
global_count = 0  # To ensure unique keys across all Training_set dictionaries

while total_count < total_needed1:
    a, b, c, d, f, g = coeff_gen()
    p_0 = 5
    roots = find_roots(a, b, c, d, f, g, p_0)
    derivative_roots = find_derivative_roots(a, b, c, d, f, g)
    x_0 = np.array([root for root in roots if root > 0])
    x_star = np.array([root for root in derivative_roots if root > 0])
    y_star = polynomial_gen(a, b, c, d, f, g, x_star)
    p_star = find_p_star(x_star, y_star) - 3
    sigma = 1.2 * x_0

    steps = 30000 #20000,30000,40000,50000
    X = simulate(steps, p_star+1, p_0, x_0, a, b, c, d, f, g)

    """
    Training_set, count = rpt_and_return(X,total_count,total_needed1)

    # Merge Training_set into All_training_set with unique keys
    for key, value in Training_set.items():
        All_training_set1[global_count] = value
        global_count += 1
    """
    All_training_set1[global_count] = X
    global_count += 1
    total_count += 1
    print(total_count)
    # total_count += count
    # print(total_count)

import matplotlib.pyplot as plt
import numpy as np

# Global font size settings
plt.rcParams.update({
    'font.size': 26,          # base font size
    'axes.titlesize': 26,     # title font size
    'axes.labelsize': 26,     # x and y axis label font size
    'xtick.labelsize': 26,    # x tick font size
    'ytick.labelsize': 26,    # y tick font size
    'legend.fontsize': 16,    # legend font size
    'figure.titlesize': 26    # figure title font size
})

t = np.linspace(0, 1, 30000)
#plt.figure(figsize=(8, 4))
plt.plot(t, All_training_set1[1])

plt.xlabel('Scaled Time')
plt.ylabel('State')
# plt.title('Example Trajectory from Training Set')  # Optional

plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Assume All_training_set1[0] is your time series
time_series = All_training_set1[0]
ts_series = pd.Series(time_series)

## Compute Rolling Variance
window_size =   1000
rolling_variance = ts_series.rolling(window=window_size, min_periods=1).var().fillna(0)

## Compute Rolling Autocorrelation as a Time Series
def rolling_autocorrelation(series, lag, window):
    return series.rolling(window).apply(lambda x: x.autocorr(lag), raw=False).fillna(0)

lag_value = 50 # Choose a lag
rolling_autocorr = rolling_autocorrelation(ts_series, lag=lag_value, window=150)

# Plot Results
plt.figure(figsize=(12, 5))

# Plot Rolling Variance
plt.subplot(1, 2, 1)
t = np.linspace(0,1,len(rolling_variance))
plt.plot(t,rolling_variance, label=f'Rolling Variance (window={window_size})', color='blue')
plt.xlabel('Scaled Time', fontsize=26)
plt.ylabel('Variance', fontsize=26)
plt.grid(True)  # Optional, adds a grid for better readability
plt.tight_layout()
#plt.title('Rolling Variance')
#plt.legend()

total_needed2 = 2
total_count = 0
All_training_set2 = {}  # Initialize as a dictionary
global_count = 0  # To ensure unique keys across all Training_set dictionaries

while total_count < total_needed2:
    a, b, c, d, f, g = coeff_gen()

    p_0 = 8
    roots = find_roots(a, b, c, d, f, g, p_0)


    x_0 = np.array([root for root in roots if root > 0])


    p_star = p_0
    sigma = 1.2 * x_0

    steps = 30000
    if random.uniform(0,1) > 0:
      X = simulate(steps, p_star, p_0, x_0, a, b, c, d, f, g)
    else:
      X = np.zeros(steps)
      for i in range(0,steps-1):
        X[i+1] = X[i] + np.sqrt(10**-2)*0.5*random.gauss(0,1)
    # plt.plot(X)

    #if rpt_and_reject(X) == 0:

    All_training_set2[global_count] = X
    global_count += 1

    total_count += 1
    print(total_count)

import matplotlib.pyplot as plt
import numpy as np

t = np.linspace(0, 1, 30000)
plt.plot(t, All_training_set2[0])

plt.xlabel('Scaled Time', fontsize=26)
plt.ylabel('State', fontsize=26)

#plt.title('Example Trajectory from Training Set')  # Optional
plt.grid(True)  # Optional, adds a grid for better readability
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Assume All_training_set1[0] is your time series
time_series = All_training_set2[0]
ts_series = pd.Series(time_series)

## Compute Rolling Variance
window_size =   1000
rolling_variance = ts_series.rolling(window=window_size, min_periods=1).var().fillna(0)

## Compute Rolling Autocorrelation as a Time Series
def rolling_autocorrelation(series, lag, window):
    return series.rolling(window).apply(lambda x: x.autocorr(lag), raw=False).fillna(0)

lag_value = 50 # Choose a lag
rolling_autocorr = rolling_autocorrelation(ts_series, lag=lag_value, window=150)

# Plot Results
plt.figure(figsize=(12, 5))

# Plot Rolling Variance
plt.subplot(1, 2, 1)
t = np.linspace(0,1,len(rolling_variance))
plt.plot(t,rolling_variance, label=f'Rolling Variance (window={window_size})', color='blue')
plt.xlabel('Scaled Time', fontsize=26)
plt.ylabel('Variance', fontsize=26)
plt.grid(True)  # Optional, adds a grid for better readability
plt.tight_layout()
plt.ylim(0, 1.2)
#plt.title('Rolling Variance')
#plt.legend()

# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt

# # Assume All_training_set1[0] is your time series
# time_series = All_training_set2[0]
# ts_series = pd.Series(time_series)

# ## Compute Rolling Variance
# window_size = 1000
# rolling_variance = ts_series.rolling(window=window_size, min_periods=1).var().fillna(0)

# ## Compute Rolling Autocorrelation as a Time Series
# def rolling_autocorrelation(series, lag, window):
#     return series.rolling(window).apply(lambda x: x.autocorr(lag), raw=False).fillna(0)

# lag_value = 1 # Choose a lag
# rolling_autocorr = rolling_autocorrelation(ts_series, lag=lag_value, window=20)

# # Plot Results
# plt.figure(figsize=(12, 5))

# # Plot Rolling Variance
# plt.subplot(1, 2, 1)
# plt.plot(rolling_variance, label=f'Rolling Variance (window={window_size})', color='blue')
# plt.xlabel('Time')
# plt.ylabel('Variance')
# plt.title('Rolling Variance')
# plt.legend()

# # Plot Rolling Autocorrelation as a Time Series
# plt.subplot(1, 2, 2)
# plt.plot(rolling_autocorr, label=f'Rolling Autocorr (lag={lag_value}, window=20)', color='red')
# plt.xlabel('Time')
# plt.ylabel('Autocorrelation')
# plt.title('Rolling Autocorrelation as a Time Series')
# plt.legend()

# plt.tight_layout()
# plt.show()

Training_set = np.zeros((total_needed2+total_needed1,30000))
Label = np.zeros((total_needed2+total_needed1,1))
for i in range(0,total_needed1):
  Training_set[i,:] = All_training_set1[i]
  Label[i] = 1
for i in range(total_needed1,total_needed1+total_needed2):
  Training_set[i,:] = All_training_set2[i-total_needed1]
  Label[i] = 0

import numpy as np
import pandas as pd

# Assume Training_set shape: (total_samples, 200) -> (N, T)
total_samples, T = Training_set.shape

# Initialize array for rolling variance
Training_variance = np.zeros_like(Training_set)  # Same shape as Training_set

# Compute rolling variance for each time series
window_size = 1000

for i in range(total_samples):
    ts_series = pd.Series(Training_set[i, :])  # Convert to pandas Series
    Training_variance[i, :] = ts_series.rolling(window=window_size, min_periods=1).var().fillna(0).values

# Concatenate original and variance data along the last axis (feature dimension)
# Shape will become (N, T, 2) where the last dimension contains (original, variance)
Training_combined = np.stack([Training_set, Training_variance], axis=-1)

# Print shapes
print("Original Training Set Shape:", Training_set.shape)  # (N, 200)
print("Rolling Variance Shape:", Training_variance.shape)  # (N, 200)
print("Combined Training Data Shape:", Training_combined.shape)  # (N, 200, 2)
print("Label Shape:", Label.shape)  # (N, 1)

shuffle_indices = np.random.permutation(total_samples)

# Apply the same shuffle to all components
Training_combined = Training_combined[shuffle_indices]  # Shuffle time series + variance
Label = Label[shuffle_indices]

import numpy as np

# Compute mean and std along the time axis (axis=1)
mean = np.mean(Training_combined, axis=1, keepdims=True)
std = np.std(Training_combined, axis=1, keepdims=True)

# Avoid division by zero
std[std == 0] = 1

# Apply z-score normalization
Training_combined = (Training_combined - mean) / std

# np.save('Training_set.npy', Training_combined)
# np.save('Label.npy', Label)
# from google.colab import files

# files.download('Training_set.npy')
# files.download('Label.npy')

from sklearn.model_selection import train_test_split

import numpy as np

# # Load data
# Training_combined = np.load("Training_set (13).npy")
# Label = np.load("Label (13).npy")

# Define split ratio
train_ratio = 0.95  # 95% for training
val_ratio = 1 - train_ratio  # 5% for validation

# Perform stratified split to maintain label distribution
X_train, X_val, y_train, y_val = train_test_split(
    Training_combined, Label, test_size=val_ratio, random_state=42, stratify=Label
)

# Print dataset sizes
print("Training Set Shape:", X_train.shape, y_train.shape)  # (1900, 200, 2) (1900, 1)
print("Validation Set Shape:", X_val.shape, y_val.shape)  # (100, 200, 2) (100, 1)

from keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import pickle
pool_size_param = 2
learning_rate_param = 0.01 #used to be 0.0005 0.001 0.005 0.01
batch_param = 32#128
dropout_percent = 0.05 #0.1
filters_param = 50 #used to be 50
mem_cells = 50 #100
mem_cells2 = 10 #50
kernel_size_param = 300 #100 is normal, tried 10 for shorter, 200 for longer and 300
epoch_param = 5 #used to be 100
initializer_param = 'lecun_normal'

model = Sequential()
model.add(Conv1D(filters=filters_param, kernel_size=kernel_size_param, activation='relu', padding='same',input_shape=(30000,2),kernel_initializer = initializer_param))
model.add(Conv1D(filters=2*filters_param, kernel_size=kernel_size_param, activation='relu', padding='same'))
model.add(Dropout(dropout_percent))
model.add(MaxPooling1D(pool_size=pool_size_param, strides=2, padding='valid'))
model.add(LSTM(mem_cells, return_sequences=True, kernel_initializer = initializer_param))
model.add(Dropout(dropout_percent))
model.add(LSTM(mem_cells2,kernel_initializer = initializer_param))
model.add(Dropout(dropout_percent))
model.add(Dense(2, activation='softmax',kernel_initializer = initializer_param)) #Dense used to be 4
model_name = 'best_model.keras'

# Set up optimiser
adam = Adam(learning_rate=learning_rate_param)
chk = ModelCheckpoint(model_name, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
early_stop = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', verbose=1)
model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy', 'sparse_categorical_accuracy'])
#model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=epoch_param, batch_size=batch_param, callbacks=[chk, early_stop], validation_data=(X_val, y_val))

#history = model.fit(Train, Train_target, epochs=epoch_param, batch_size=batch_param, callbacks=[chk],validation_data=(Validation,Validation_target))

model = load_model(model_name)

from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    f1_score
)
# --- Evaluate on Validation Set ---
val_loss, val_acc, val_sparse_acc = model.evaluate(X_val, y_val, verbose=0)
print(f"Validation Accuracy (Keras): {val_acc:.4f}")

# --- Predict Classes ---
y_pred_probs = model.predict(X_val)
y_pred = np.argmax(y_pred_probs, axis=1)

# --- Compute Metrics ---
acc = accuracy_score(y_val, y_pred)
f1_macro = f1_score(y_val, y_pred, average='macro')
f1_micro = f1_score(y_val, y_pred, average='micro')
f1_weighted = f1_score(y_val, y_pred, average='weighted')

print("\n--- Evaluation Metrics ---")
print(f"Accuracy: {acc:.4f}")
print(f"F1 Score (macro): {f1_macro:.4f}")
print(f"F1 Score (micro): {f1_micro:.4f}")
print(f"F1 Score (weighted): {f1_weighted:.4f}")

# --- Confusion Matrix ---
print("\nConfusion Matrix:")
print(confusion_matrix(y_val, y_pred))

# --- Detailed Classification Report ---
print("\nClassification Report:")
print(classification_report(y_val, y_pred, digits=3))

# --- Save Metrics to CSV (Optional) ---
metrics_df = pd.DataFrame({
    'accuracy': [acc],
    'f1_macro': [f1_macro],
    'f1_micro': [f1_micro],
    'f1_weighted': [f1_weighted],
    'val_loss': [val_loss],
    'val_accuracy': [val_acc]
})
metrics_df.to_csv("evaluation_metrics.csv", index=False)

# --- Save Training History ---
with open('training_history.pkl', 'wb') as f:
    pickle.dump(history.history, f)

# Optional: Print training history
print("\n--- Training History ---")
for key, values in history.history.items():
    print(f"{key}: {values}")

model.save('flickering_4000_time_series_30000.h5')

from google.colab import files
files.download('flickering_40_time_series_9000.h5')
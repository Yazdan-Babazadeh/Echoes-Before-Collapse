# -*- coding: utf-8 -*-
"""flickering big plot .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DGES14Jg1f4M6mUT_AZaBsrorrmugxra
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import pandas as pd

from google.colab import files
uploaded = files.upload()

from tensorflow.keras.models import load_model
model1 = load_model('flickering_4000_time_series_5000.h5')
model2 = load_model('flickering_4000_time_series_6000.h5')
model3 = load_model('flickering_4000_time_series_7000.h5')
model4 = load_model('flickering_4000_time_series_8000.h5')
model5 = load_model('flickering_4000_time_series_9000.h5')
model6 = load_model('flickering 4000 time_series_10000.h5')

def ts_gen(index_model,mode):
  if index_model==1 and mode ==1: #############################################
    steps = 62500
    dt = 10**-2

    X = np.zeros(steps)
    c = 0
    d = 1.5
    b = 0.5

    x_star = (1/3)*(c + np.sqrt((c**2) + (3*(d-1))))
    y_star = -(x_star**3) + (c*x_star**2) + (d*x_star)

    b_final = x_star - y_star - 0.5

    delta_b = (b_final - b) / steps


    def x_dot(x, b):
        return b - x - x**3 + (c*x**2) + (d*x)

    X[0] = 1
    S = 0.4#0.5
    window_size = 1000
    # Generate the time series
    for i in range(0, steps-1):
        X[i+1] = X[i] + (x_dot(X[i], b)*dt) + (np.sqrt(dt)*S*random.gauss(0, 1))
        b = b + delta_b

    X_series = pd.Series(X)

    # Compute rolling variance with the same window size
    X_var = X_series.rolling(window=window_size, min_periods=1).var().fillna(0).values

    return X

  elif index_model==1 and mode == 0: ##########################################
    steps = 62500
    dt = 10**-2

    X = np.zeros(steps)
    c = 0
    d = 1.5
    b = 0.5  # constant b

    x_star = (1/3)*(c + np.sqrt((c**2) + (3*(d-1))))
    y_star = -(x_star**3) + (c*x_star**2) + (d*x_star)

    # b stays constant → no delta_b needed
    def x_dot(x, b):
        return b - x - x**3 + (c*x**2) + (d*x)

    X[0] = 1
    S_initial = 0.4
    S_max = 1.0  # choose peak value

    # Precompute S over time
    S_values = np.zeros(steps)
    one_third = steps // 3

    # First third: constant S_initial
    S_values[:one_third] = S_initial

    # Middle third: ramp up then down (triangle shape)
    mid_steps = one_third
    for i in range(mid_steps):
        if i < mid_steps // 2:
            # Increase from initial to max
            S_values[one_third + i] = S_initial + (S_max - S_initial) * (i / (mid_steps // 2))
        else:
            # Decrease back to initial
            S_values[one_third + i] = S_max - (S_max - S_initial) * ((i - mid_steps // 2) / (mid_steps // 2))

    # Last third: constant S_initial
    S_values[-one_third:] = S_initial

    # Simulation loop
    for i in range(steps - 1):
        S = S_values[i]
        X[i+1] = X[i] + (x_dot(X[i], b) * dt) + (np.sqrt(dt) * S * random.gauss(0, 1))

    # Convert to pandas Series
    X_series = pd.Series(X)

    # Rolling variance
    window_size = 1000
    X_var = X_series.rolling(window=window_size, min_periods=1).var().fillna(0).values
    t = np.linspace(0, 1, steps)
    X1 = X
    return X1

  elif index_model==2 and mode ==1: ###########################################
    steps = 62500  # Number of steps
    dt = 1e-2       # Time step
    alpha_start = 0 # Starting value of alpha
    alpha_end = -1  # Ending value of alpha
    S = 1           # s parameter
    r = 2           # r parameter
    b = 2         # b parameter for the new term
    sigma = 0.45   # sigma (noise strength)
    P0 = 2        # Initial value of P

    # Precompute alpha values decreasing linearly
    alpha_values = np.linspace(alpha_start, alpha_end, steps)

    d_alpha = (alpha_end - alpha_start) / steps

    # Initialize arrays
    P3 = np.zeros(steps)
    P3[0] = P0

    # Define the drift and diffusion terms
    def drift(P, alpha, S, r, b):
        return alpha - (S * P) + r * (1 - np.exp(-b * P**2))


    alpha = alpha_start
    # Simulate the SDE using Euler-Maruyama method
    for i in range(steps - 1):
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S, r, b) * dt + sigma * np.sqrt(dt)*random.gauss(0, 1)
        alpha = alpha + d_alpha

    return P3

  elif index_model == 2 and mode ==0: ########################################

    steps = 62500  # Number of steps
    dt = 1e-2      # Time step
    alpha = 0      # Keep alpha constant
    S = 1          # S parameter
    r = 2          # r parameter
    b = 2          # b parameter for the new term
    sigma_initial = 0.45  # Initial noise strength
    sigma_max = 1.0        # Peak noise strength
    P0 = 2                 # Initial value of P

    # Precompute sigma values over time
    sigma_values = np.zeros(steps)
    one_third = steps // 3

    # First third: constant sigma_initial
    sigma_values[:one_third] = sigma_initial

    # Middle third: ramp up to max, then ramp down
    mid_steps = one_third
    for i in range(mid_steps):
        if i < mid_steps // 2:
            # Increase from initial to max
            sigma_values[one_third + i] = sigma_initial + (sigma_max - sigma_initial) * (i / (mid_steps // 2))
        else:
            # Decrease back to initial
            sigma_values[one_third + i] = sigma_max - (sigma_max - sigma_initial) * ((i - mid_steps // 2) / (mid_steps // 2))

    # Last third: constant sigma_initial
    sigma_values[-one_third:] = sigma_initial

    # Initialize array for P values
    P3 = np.zeros(steps)
    P3[0] = P0

    # Define the drift term (alpha is constant here)
    def drift(P, alpha, S, r, b):
        return alpha - (S * P) + r * (1 - np.exp(-b * P**2))

    # Simulate the SDE using Euler–Maruyama
    for i in range(steps - 1):
        sigma = sigma_values[i]
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S, r, b) * dt + sigma * np.sqrt(dt) * random.gauss(0, 1)
    X2 = P3
    return X2

  elif index_model == 3 and mode ==1: #####################################################
    steps = 62500  # Number of steps
    dt = 1e-2       # Time step
    alpha_start = 0.5 # Starting value of alpha
    alpha_end = -1 # Ending value of alpha
    S = 1         # s parameter
    r = 2          # r parameter
    sigma = 0.9  # sigma (noise strength)
    P0 = 3       # Initial value of P

    # Precompute alpha values decreasing linearly

    alpha_values = np.linspace(alpha_start, alpha_end, steps)


    # Initialize arrays
    P2 = np.zeros(steps)
    P2[0] = P0

    # Define the drift and diffusion terms
    def drift(P, alpha, S, r):
        return alpha - S * P + (r * np.tanh(P))

    def diffusion(P, sigma):
        return sigma

    # Simulate the SDE using Euler-Maruyama method
    for i in range(steps - 1):
        dW = np.sqrt(dt) * random.gauss(0, 1)  # Wiener increment
        alpha = alpha_values[i]
        P2[i+1] = P2[i] + drift(P2[i], alpha, S, r) * dt + diffusion(P2[i], sigma) * dW


    return P2


  elif index_model == 3 and mode ==0: #####################################################
    steps = 62500   # Number of steps
    dt = 1e-2       # Time step
    alpha = 0.5     # Keep alpha constant
    S = 1           # s parameter
    r = 2           # r parameter
    sigma_initial = 0.9  # Initial noise strength
    sigma_max = 1.5       # Peak noise strength
    P0 = 3               # Initial value of P

    # Precompute sigma variation over time
    sigma_values = np.zeros(steps)
    one_third = steps // 3

    # First third: constant
    sigma_values[:one_third] = sigma_initial

    # Middle third: ramp up then down
    mid_steps = one_third
    for i in range(mid_steps):
        if i < mid_steps // 2:
            sigma_values[one_third + i] = sigma_initial + (sigma_max - sigma_initial) * (i / (mid_steps // 2))
        else:
            sigma_values[one_third + i] = sigma_max - (sigma_max - sigma_initial) * ((i - mid_steps // 2) / (mid_steps // 2))

    # Last third: constant
    sigma_values[-one_third:] = sigma_initial

    # Initialize array
    P2 = np.zeros(steps)
    P2[0] = P0

    # Drift term
    def drift(P, alpha, S, r):
        return alpha - S * P + (r * np.tanh(P))

    # Simulate Euler–Maruyama
    for i in range(steps - 1):
        dW = np.sqrt(dt) * random.gauss(0, 1)  # Wiener increment
        sigma = sigma_values[i]
        P2[i+1] = P2[i] + drift(P2[i], alpha, S, r) * dt + sigma * dW
    X3 = P2
    return X3


  elif index_model ==4 and mode == 1: ################################################################################
    steps = 62500  # Number of steps
    dt = 1e-2       # Time step
    alpha_start = 1 # Starting value of alpha
    alpha_end = -0.5 # Ending value of alpha
    S = 1           # s parameter
    r = 1.5           # r parameter
    b = 2         # b parameter for the new term
    sigma = 0.5   # sigma (noise strength)
    P0 = 2        # Initial value of P

    # Precompute alpha values decreasing linearly
    alpha_values = np.linspace(alpha_start, alpha_end, steps)

    d_alpha = (alpha_end - alpha_start) / steps

    # Initialize arrays
    P3 = np.zeros(steps)
    P3[0] = P0

    # Define the drift and diffusion terms
    def drift(P, alpha, S, r, b):
        return alpha - (S * P) + r * (P**6/(1+P**6))


    alpha = alpha_start
    # Simulate the SDE using Euler-Maruyama method
    for i in range(steps - 1):
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S, r, b) * dt + sigma * np.sqrt(dt)*random.gauss(0, 1)
        alpha = alpha + d_alpha

    return P3

  elif index_model ==4 and mode == 0: ################################################################################

    steps = 62500  # Number of steps
    dt = 1e-2      # Time step
    alpha = 1      # Keep alpha constant (initial value)
    S = 1          # s parameter
    r = 1.5        # r parameter
    b = 2          # b parameter for the new term
    sigma_initial = 0.5  # Initial noise strength
    sigma_max = 1.0       # Peak noise strength
    P0 = 2               # Initial value of P

    # Create sigma variation pattern
    sigma_values = np.zeros(steps)
    one_third = steps // 3

    # First third: constant
    sigma_values[:one_third] = sigma_initial

    # Middle third: ramp up then down
    mid_steps = one_third
    for i in range(mid_steps):
        if i < mid_steps // 2:
            sigma_values[one_third + i] = sigma_initial + (sigma_max - sigma_initial) * (i / (mid_steps // 2))
        else:
            sigma_values[one_third + i] = sigma_max - (sigma_max - sigma_initial) * ((i - mid_steps // 2) / (mid_steps // 2))

    # Last third: constant
    sigma_values[-one_third:] = sigma_initial

    # Initialize array
    P3 = np.zeros(steps)
    P3[0] = P0

    # Drift term
    def drift(P, alpha, S, r, b):
        return alpha - (S * P) + r * (P**6 / (1 + P**6))

    # Simulate SDE using Euler–Maruyama
    for i in range(steps - 1):
        dW = np.sqrt(dt) * random.gauss(0, 1)  # Wiener increment
        sigma = sigma_values[i]
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S, r, b) * dt + sigma * dW
    X4 = P3
    return X4


  elif index_model ==5 and mode == 1: #########################################
    steps = 62500  # Number of steps
    dt = 1e-2       # Time step
    alpha_start = 0 # Starting value of alpha
    alpha_end = -1 # Ending value of alpha
    S = 1           # s parameter
            # b parameter for the new term
    sigma = 0.3   # sigma (noise strength)
    P0 = 1        # Initial value of P

    # Precompute alpha values decreasing linearly
    alpha_values = np.linspace(alpha_start, alpha_end, steps)

    d_alpha = (alpha_end - alpha_start) / steps

    # Initialize arrays
    P3 = np.zeros(steps)
    P3[0] = P0

    # Define the drift and diffusion terms
    def drift(P, alpha, S):
        return alpha - (S * P) + (1/(1+np.exp(-10*P)))


    alpha = alpha_start
    # Simulate the SDE using Euler-Maruyama method
    for i in range(steps - 1):
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S) * dt + sigma * np.sqrt(dt)*random.gauss(0, 1)
        alpha = alpha + d_alpha


    return P3

  elif index_model ==5 and mode == 0: #########################################
    steps = 62500   # Number of steps
    dt = 1e-2       # Time step
    alpha = 0       # Keep alpha constant
    S = 1           # s parameter
    sigma_initial = 0.3   # Initial noise strength
    sigma_max = 0.8       # Peak noise strength
    P0 = 1                # Initial value of P

    # Create sigma variation pattern
    sigma_values = np.zeros(steps)
    one_third = steps // 3

    # First third: constant
    sigma_values[:one_third] = sigma_initial

    # Middle third: ramp up then down
    mid_steps = one_third
    for i in range(mid_steps):
        if i < mid_steps // 2:
            sigma_values[one_third + i] = sigma_initial + (sigma_max - sigma_initial) * (i / (mid_steps // 2))
        else:
            sigma_values[one_third + i] = sigma_max - (sigma_max - sigma_initial) * ((i - mid_steps // 2) / (mid_steps // 2))

    # Last third: constant
    sigma_values[-one_third:] = sigma_initial

    # Initialize array
    P3 = np.zeros(steps)
    P3[0] = P0

    # Drift term
    def drift(P, alpha, S):
        return alpha - (S * P) + (1 / (1 + np.exp(-10 * P)))

    # Simulate Euler–Maruyama
    for i in range(steps - 1):
        dW = np.sqrt(dt) * random.gauss(0, 1)  # Wiener increment
        sigma = sigma_values[i]
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S) * dt + sigma * dW
    X5 = P3
    return X5

  elif index_model ==6 and mode ==1:
    steps = 62500  # Number of steps
    dt = 1e-2       # Time step
    alpha_start = 0.5 # Starting value of alpha
    alpha_end = -1 # Ending value of alpha
    S = 1           # s parameter
    r = 1
    b = 1       # b parameter for the new term
    sigma = 0.8   # sigma (noise strength)
    P0 = 2        # Initial value of P

    # Precompute alpha values decreasing linearly
    alpha_values = np.linspace(alpha_start, alpha_end, steps)

    d_alpha = (alpha_end - alpha_start) / steps

    # Initialize arrays
    P3 = np.zeros(steps)
    P3[0] = P0

    # Define the drift and diffusion terms
    def drift(P, alpha, S, r, b):
        return alpha - (S * P) + (np.arctan(10*P))


    alpha = alpha_start

    for i in range(steps - 1):
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S, r, b) * dt + sigma * np.sqrt(dt)*random.gauss(0, 1)
        alpha = alpha + d_alpha

    return P3
  elif index_model ==6 and mode ==0:

    steps = 62500    # Number of steps
    dt = 1e-2        # Time step
    alpha = 0.5      # Keep alpha constant
    S = 1            # s parameter
    r = 1
    b = 1
    sigma_initial = 0.8   # Initial noise strength
    sigma_max = 1.5        # Peak noise strength
    P0 = 2                 # Initial value of P

    # Create sigma variation pattern (triangle shape)
    sigma_values = np.zeros(steps)
    one_third = steps // 3

    # First third: constant
    sigma_values[:one_third] = sigma_initial

    # Middle third: ramp up then down
    mid_steps = one_third
    for i in range(mid_steps):
        if i < mid_steps // 2:
            sigma_values[one_third + i] = sigma_initial + (sigma_max - sigma_initial) * (i / (mid_steps // 2))
        else:
            sigma_values[one_third + i] = sigma_max - (sigma_max - sigma_initial) * ((i - mid_steps // 2) / (mid_steps // 2))

    # Last third: constant
    sigma_values[-one_third:] = sigma_initial

    # Initialize arrays
    P3 = np.zeros(steps)
    P3[0] = P0

    # Drift term
    def drift(P, alpha, S, r, b):
        return alpha - (S * P) + np.arctan(10 * P)

    # Simulate Euler–Maruyama method
    for i in range(steps - 1):
        dW = np.sqrt(dt) * random.gauss(0, 1)  # Wiener increment
        sigma = sigma_values[i]
        P3[i + 1] = P3[i] + drift(P3[i], alpha, S, r, b) * dt + sigma * dW
    X6 = P3
    return X6

def dl_analysis(X):
  Steps = [5000,6000,7000,8000,9000,10000]
  Model = [model1,model2,model3,model4,model5,model6]
  def analysis(model,X3,index):

    import statsmodels.api as sm
    from scipy.ndimage import gaussian_filter1d
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
    from statsmodels.tsa.seasonal import seasonal_decompose
    steps2 = len(X3)
    steps = Steps[index]
    intervals = 1000


    size = int(((steps2/steps)-1)*intervals)+1
    Y = np.zeros((size,steps,2))

    t = np.zeros(size)
    for i in range(0,size):
      j1=int(i*steps/intervals)
      j2 = j1+steps

      Y[i,:,0] = X3[j1:j2]
      Y[i,:,0] = (Y[i,:,0]-np.average(Y[i,:,0]))/(np.std(Y[i,:,0]))
      X_series = pd.Series(X3[j1:j2])
      window_size = 1000
      X_var = X_series.rolling(window=window_size, min_periods=1).var().fillna(0).values
      Y[i,:,1] = X_var
      Y[i,:,1] = (Y[i,:,1]-np.average(Y[i,:,1]))/(np.std(Y[i,:,1]))


    np.shape(Y)
    predict_x1=model.predict(Y)



    non_flicker_prob1 = predict_x1[:,0]
    flicker_prob1 = predict_x1[:,1]

    window_size = 400

    non_flicker_prob2 = np.convolve(non_flicker_prob1, np.ones(window_size)/window_size, mode='valid')
    flicker_prob2 = np.convolve(flicker_prob1, np.ones(window_size)/window_size, mode='valid')


    t = np.linspace(0, 1 - (steps/steps2), len(non_flicker_prob2))
    return t,flicker_prob2,non_flicker_prob2
###############################
  T = []
  Flicker_Prob = []
  Non_Flicker_Prob = []

  for i in range(0,6):
      t,flicker_prob2,non_flicker_prob2 = analysis(Model[i],X,i)
      T.append(t)
      Flicker_Prob.append(flicker_prob2)
      Non_Flicker_Prob.append(non_flicker_prob2)

  from collections import defaultdict


  time_dict = defaultdict(list)

  for t_series, prob_series in zip(T, Flicker_Prob):
      for t_val, prob_val in zip(t_series, prob_series):
            t_key = round(t_val, 3)  # round to avoid float precision issues
            time_dict[t_key].append(prob_val)

    # Aggregate
  common_T = sorted(time_dict.keys())
  mean_probs = [np.mean(time_dict[t]) for t in common_T]

    # Plot

  return common_T,mean_probs

def moving_variance(x, window):
    w = int(window)
    if w < 1 or w > len(x):
        raise ValueError("window must be in [1, len(x)]")
    kernel = np.ones(w) / w
    mean = np.convolve(x, kernel, mode='valid')
    mean2 = np.convolve(x**2, kernel, mode='valid')
    var = mean2 - mean**2
    # align time for the 'valid' window
    t_var = np.linspace((w-1)/(len(x)-1), 1.0, len(var))
    return t_var, var

import numpy as np
import matplotlib.pyplot as plt

# --- generate series ---
X = ts_gen(6, 0)
t = np.linspace(0,1,len(X))
plt.plot(t,X)              # your function

# --- plotting ---

Time,scores = dl_analysis(X)        # your function; assumed to return values in [0,1]



# --- time axis scaled to [0,1] ---
N = len(X)
t = np.linspace(0, 1, N)



w = 500  # <-- set your preferred window size
t_var, varX = moving_variance(X, w)

fig, axes = plt.subplots(3, 1, figsize=(10, 9), sharex=True)
scores = np.array(scores)
# 1) raw X
axes[0].plot(t, X, linewidth=1)
axes[0].set_ylabel("X")
axes[0].set_title("Time series and analyses (time scaled to [0,1])")

# 2) dl_analysis and its complement
axes[1].plot(Time, scores, label="dl_analysis(X)", linewidth=1)
axes[1].plot(Time, 1 - scores, label="1 - dl_analysis(X)", linewidth=1, linestyle="--")
axes[1].set_ylabel("Score")
axes[1].legend(loc="best", frameon=False)

# 3) moving variance
axes[2].plot(t_var, varX, linewidth=1)
axes[2].set_xlabel("Scaled time (0 → 1)")
axes[2].set_ylabel(f"Var(X), w={w}")
axes[2].ylim(0,0.4)

plt.tight_layout()
plt.show()

import numpy as np
from google.colab import files

# Make sure scores is a NumPy array (not a list)
scores = np.asarray(scores)

# Save everything into one compressed NPZ
np.savez_compressed(
    "no flicker 6.npz",
    t=t,              # raw time for X
    X=X,              # raw series
    Time=Time,        # time axis for scores (may be shorter than t)
    scores=scores     # dl_analysis output
)

# Download in Colab
files.download("no flicker 6.npz")

from google.colab import files
uploaded = files.upload()

data = np.load("flicker 1.npz")
t11 = data["t"]
X11 = data["X"]
Time11 = data["Time"]
scores11 = data["scores"]

data = np.load("no flicker 1.npz")
t10 = data["t"]
X10 = data["X"]
Time10 = data["Time"]
scores10 = data["scores"]

data = np.load("flicker 2.npz")
t21 = data["t"]
X21 = data["X"]
Time21 = data["Time"]
scores21 = data["scores"]

data = np.load("no flicker 2.npz")
t20 = data["t"]
X20 = data["X"]
Time20 = data["Time"]
scores20 = data["scores"]

data = np.load("flicker 3.npz")
t31 = data["t"]
X31 = data["X"]
Time31 = data["Time"]
scores31 = data["scores"]

data = np.load("no flicker 3.npz")
t30 = data["t"]
X30 = data["X"]
Time30 = data["Time"]
scores30 = data["scores"]

data = np.load("flicker 4.npz")
t41 = data["t"]
X41 = data["X"]
Time41 = data["Time"]
scores41 = data["scores"]

data = np.load("no flicker 4.npz")
t40 = data["t"]
X40 = data["X"]
Time40 = data["Time"]
scores40 = data["scores"]

data = np.load("flicker 5.npz")
t51 = data["t"]
X51 = data["X"]
Time51 = data["Time"]
scores51 = data["scores"]

data = np.load("no flicker 5.npz")
t50 = data["t"]
X50 = data["X"]
Time50 = data["Time"]
scores50 = data["scores"]

data = np.load("flicker 6.npz")
t61 = data["t"]
X61 = data["X"]
Time61 = data["Time"]
scores61 = data["scores"]

data = np.load("no flicker 6.npz")
t60 = data["t"]
X60 = data["X"]
Time60 = data["Time"]
scores60 = data["scores"]

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Patch

# ===================== KNOBS (size controls) =====================
FIGSIZE = (18, 22)
DPI     = 600

# Outer 3x2 panel sizing (models)
OUTER_HEIGHT_RATIOS = [1, 1, 1]
OUTER_WIDTH_RATIOS  = [1.0, 1.0]
OUTER_HSPACE = 0.28
OUTER_WSPACE = 0.12

# Inner per-model panel sizing (3 rows x 2 cols)
INNER_HEIGHT_RATIOS = [3, 2.5, 1]
INNER_WIDTH_RATIOS  = [1.0, 1.0]
INNER_HSPACE = 0.06
INNER_WSPACE = 0.08

# Analysis knobs
VAR_WINDOW   = 1000
SHADE_RANGE  = (0.38, 0.60)
SHADE_ALPHA  = 0.18

# ---- Style knobs you asked for ----
GRAYS = ["#333333", "#4D4D4D", "#666666", "#808080", "#999999", "#B3B3B3"]
TS_COLOR            = GRAYS[1]
DL_FLICKER_COLOR    = "blue"     # change to '#1f77b4' if you prefer mpl default
DL_NONFLICKER_COLOR = '#E69F00'     # 'yellow' works too; 'gold' has better contrast
DL_LINEWIDTH        = 1        # <-- set to 1.0 if you want even thinner
VAR_COLOR           = "#7A7A7A"  # neutral gray
SHADE_COLOR         = "red"      # keep as-is
# ================================================================

# ---- pack your already-loaded arrays ----
models = [
    ((t11, X11, Time11, scores11), (t10, X10, Time10, scores10)),
    ((t21, X21, Time21, scores21), (t20, X20, Time20, scores20)),
    ((t31, X31, Time31, scores31), (t30, X30, Time30, scores30)),
    ((t41, X41, Time41, scores41), (t40, X40, Time40, scores40)),
    ((t51, X51, Time51, scores51), (t50, X50, Time50, scores50)),
    ((t61, X61, Time61, scores61), (t60, X60, Time60, scores60)),
]

# Equations (order matches models)
equations = [
    r"$\frac{dx}{dt} = b - x + D x - x^{3}$",
    r"$\frac{dx}{dt} = b - x + r\,\left(1 - e^{-c x^{2}}\right)$",
    r"$\frac{dx}{dt} = b - x + r\,\tanh(x)$",
    r"$\frac{dx}{dt} = b - x + r\,\frac{x^{6}}{1 + x^{6}}$",
    r"$\frac{dx}{dt} = b - x + \frac{1}{1 + e^{-r x}}$",
    r"$\frac{dx}{dt} = b - x + \arctan(r x)$",
]

def moving_variance_same(x, w=1000):
    x = np.asarray(x, dtype=float)
    w = int(w)
    if w < 1:
        raise ValueError("window must be >= 1")
    kernel = np.ones(w) / w
    mean  = np.convolve(x, kernel, mode='same')
    mean2 = np.convolve(x*x, kernel, mode='same')
    var = mean2 - mean**2
    return np.maximum(var, 0.0)

# -------- figure & outer layout --------
fig = plt.figure(figsize=FIGSIZE, dpi=DPI, constrained_layout=False)
outer = fig.add_gridspec(
    3, 2,
    height_ratios=OUTER_HEIGHT_RATIOS,
    width_ratios=OUTER_WIDTH_RATIOS,
    hspace=OUTER_HSPACE,
    wspace=OUTER_WSPACE
)

top_axes_pairs = []
legend_handles = None
legend_labels  = None

for m in range(6):
    row, col = divmod(m, 2)
    inner = outer[row, col].subgridspec(
        3, 2,
        height_ratios=INNER_HEIGHT_RATIOS,
        width_ratios=INNER_WIDTH_RATIOS,
        hspace=INNER_HSPACE,
        wspace=INNER_WSPACE
    )

    ax_ts_f   = fig.add_subplot(inner[0, 0])
    ax_ts_nf  = fig.add_subplot(inner[0, 1], sharey=ax_ts_f)
    ax_dl_f   = fig.add_subplot(inner[1, 0])
    ax_dl_nf  = fig.add_subplot(inner[1, 1], sharey=ax_dl_f)
    ax_var_f  = fig.add_subplot(inner[2, 0], sharex=ax_ts_f)
    ax_var_nf = fig.add_subplot(inner[2, 1], sharex=ax_ts_nf, sharey=ax_var_f)

    top_axes_pairs.append((ax_ts_f, ax_ts_nf))

    (t_f, X_f, Time_f, s_f), (t_nf, X_nf, Time_nf, s_nf) = models[m]
    s_f = np.asarray(s_f); s_nf = np.asarray(s_nf)

    # --- Time series (BLACK) ---
    ax_ts_f.plot(t_f,  X_f, linewidth=0.9, color=TS_COLOR)
    ax_ts_nf.plot(t_nf, X_nf, linewidth=0.9, color=TS_COLOR)
    ax_ts_nf.tick_params(labelleft=False)

    y_min = min(np.min(X_f), np.min(X_nf))
    y_max = max(np.max(X_f), np.max(X_nf))
    pad = 0.05 * (y_max - y_min + 1e-12)
    ax_ts_f.set_ylim(y_min - pad, y_max + pad)

    ax_ts_f.set_ylabel("X")
    ax_ts_f.tick_params(labelbottom=False)
    ax_ts_nf.tick_params(labelbottom=False)

    ax_ts_f.set_title("Flickering", fontsize=11)
    ax_ts_nf.set_title("Noise-Modulated (Null)", fontsize=11)

    # --- DL analysis (THINNER BLUE & YELLOW) ---
    ax_dl_f.plot(Time_f, s_f,                linewidth=DL_LINEWIDTH, color=DL_FLICKER_COLOR)
    ax_dl_f.plot(Time_f, 1 - s_f, linestyle="--", linewidth=DL_LINEWIDTH, color=DL_NONFLICKER_COLOR)
    ax_dl_f.set_ylim(0, 1)
    ax_dl_f.set_ylabel("Probability")
    ax_dl_f.tick_params(labelbottom=False)

    line1 = ax_dl_nf.plot(Time_nf, s_nf,                linewidth=DL_LINEWIDTH, color=DL_FLICKER_COLOR)[0]
    line2 = ax_dl_nf.plot(Time_nf, 1 - s_nf, linestyle="--", linewidth=DL_LINEWIDTH, color=DL_NONFLICKER_COLOR)[0]
    if m == 0:
        line1.set_label("Flicker probability")
        line2.set_label("Non-flicker probability")
        legend_handles, legend_labels = ax_dl_nf.get_legend_handles_labels()
    ax_dl_nf.tick_params(labelleft=False, labelbottom=False)

    # --- Moving variance (neutral gray) ---
    var_f  = moving_variance_same(X_f,  VAR_WINDOW)
    var_nf = moving_variance_same(X_nf, VAR_WINDOW)

    ax_var_nf.tick_params(labelleft=False)

    vmin = min(np.min(var_f), np.min(var_nf))
    vmax = max(np.max(var_f), np.max(var_nf))
    vpad = 0.05 * (vmax - vmin + 1e-12)
    ax_var_f.set_ylim(vmin - vpad, vmax + vpad)

    ax_var_f.plot(t_f,  var_f,  linewidth=0.9, color=VAR_COLOR)
    ax_var_nf.plot(t_nf, var_nf, linewidth=0.9, color=VAR_COLOR)
    ax_var_nf.axvspan(SHADE_RANGE[0], SHADE_RANGE[1], alpha=SHADE_ALPHA, color=SHADE_COLOR, linewidth=0)

    ax_var_f.set_ylabel("Variance", labelpad=6)
    ax_var_f.set_xlabel("Scaled Time")
    ax_var_nf.set_xlabel("Scaled Time")
    for a in (ax_var_f, ax_var_nf):
        a.grid(False)

plt.tight_layout(rect=[0.03, 0.06, 0.97, 0.99])

fig.canvas.draw()
for i, (ax_left, ax_right) in enumerate(top_axes_pairs, start=1):
    box_l = ax_left.get_position()
    box_r = ax_right.get_position()
    x_center = 0.5 * (box_l.x0 + box_r.x1)
    y_top    = max(box_l.y1, box_r.y1)
    title_eq = f"Model {i}: " + equations[i-1]
    fig.text(x_center, y_top + 0.012, title_eq,
             ha="center", va="bottom", fontsize=12, fontweight="bold")

if legend_handles is not None:
    band = Patch(facecolor=SHADE_COLOR, alpha=SHADE_ALPHA,
                 label='False-alarm window (variance only)')
    fig.legend(legend_handles + [band],
               legend_labels  + [band.get_label()],
               loc="lower center", ncol=3, frameon=False, bbox_to_anchor=(0.5, 0.065))

plt.suptitle("Deep-Learning Analysis of True Flickering vs Noise-Modulated (Null) Dynamics",
             fontsize=16, y=0.93)
pdf_path = "flicker_vs_null_six_models.pdf"
fig.savefig(pdf_path, format="pdf", bbox_inches="tight")
from google.colab import files; files.download(pdf_path)
plt.show()